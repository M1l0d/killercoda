We will now need to update the run function you previously wrote to log metrics


```
def run_model():
    #new
    #--
    global model_run_counter, accuracy_metric, prediction_counter 
    #--

    # Random selection of testing data
    test_split_ratio = random.uniform(0.1, 0.3)

    # Training and testing sets
    features_train, features_test, labels_train, labels_test = train_test_split(iris.data, iris.target, test_size=test_split_ratio)

    # Train the model on the training data
    model.fit(features_train, labels_train)

    # Make predictions on the test data
    test_predictions = model.predict(features_test)

    #new
    #---
    model_run_counter.inc()  # Increment the model run count
    prediction_counter.inc(len(predictions))  # Increment predictions count
    #--

    # Calculate the accuracy
    accuracy_score = model.score(features_test, labels_test)

    #new
    #---
    accuracy_metric.set(accuracy)  # Log accuracy
    #--

    #remove old print statements and add this
    #--
    print(f'Running count for Model runs: {model_run_counter._value.get()}')  # Print current count
    print(f'Accuravy of current model: {accuracy}')  # Print accuracy
    print(f'Predictions: {predictions}')  # Print predictions
    print(f'Test size: {test_size}')  # Print the test size used
    #--

```
